1+1
1Â´1
1+1
clear
print("hello")
print(hello)
print("hello")
1+1*2
# Ausgabe
print("hello")
# Rechnung
1+1*2
## Variable
var_1 = 1
var_2 = 1.5
var_3 = "Hello"
print(var1+(2*var_2))
var_1 = 1
var_2 = 1.5
var_3 = "Hello"
print(var_1+(2*var_2))
# Class check
class(var_4_2)
# Int
var_1 = 1
# Dec
var_2 = 1.5
# String
var_3 = "Hello"
# list
var_4_1 = c(1,2,3,4,5)
var_4_2 = c("Hello", "Cheese", "Tomato")
# Boolean
var_5 = TRUE
## calc during print
print(var_1+(2*var_2))
# Class check
class(var_4_2)
# Class check
class(var_2)
# Class check
class(var_1)
# Rechnung
1+1*2
rm(list=ls())
setwd("~/OneDrive - BI Norwegian Business School (BIEDU)/R-Studio")
import(iris)
import("iris")
iris
iris_dataset = (iris)
pwd
path
write.csv2(iris,"~/Library/CloudStorage/OneDrive-BINorwegianBusinessSchool(BIEDU)/R-Studio/R Intro Course/iris.csv")
iris_dataset_from_csv = read.csv2("/R Intro Course/iris.csv")
iris_dataset_from_csv = read.csv2("~/Library/CloudStorage/OneDrive-BINorwegianBusinessSchool(BIEDU)/R-Studio/R Intro Course/iris.csv")
view(iris_dataset_from_csv)
View()
View(iris_dataset_from_csv)
install.packages("readxl")
install.packages(c("boot", "Matrix"), lib="/opt/homebrew/Cellar/r/4.5.1/lib/R/library")
# use package
library(readxl)
install.packages("quantmod")
library(quantmod)
getSymbols(tickers)
tickers = c("AAPL","TSLA")
getSymbols(tickers)
View(AAPL)
tickers = c("AAPL","TSLA", "7011.TY", "1.HK")
getSymbols(tickers)
tickers = c("AAPL","TSLA", "7011.T", "0001.HK")
getSymbols(tickers)
View(`0001.HK`)
getSymbols("DEXUSEU", src = "ECB")
getQuote(tickers)
tickers = c("AAPL","TSLA", "7011.T", "0001.HK")
getSymbols(tickers)
getQuote(tickers)
getMetals("XAG")
getFX("USD/EUR")
View(TSLA)
View(TSLA)
# Data manipulation
compare_Stock = merge(
TSLA = [, "TSLA.Close"],
install.packages("dplyr")
library(dplyr)
compare_Stock = merge(
TSLA [, "TSLA.Close"],
AAPL [, "AAPL.Close"],
join = "Time"
)
compare_Stock = merge(
TSLA [, "TSLA.Close"],
AAPL [, "AAPL.Close"],
join = "inner"
)
View(compare_Stock)
View(compare_Stock)
compare_stock_df = as.data.frame(compare_Stock)
View(compare_stock_df)
iris_dataset = (iris)
plot("TSLA.Close","AAPL.Close")
plot("Sepal.Length,"Sepal.Width")
plot("Sepal.Length","Sepal.Width")
mean_tsla_close = mean(TSLA.Close)
mean_tsla_close = mean("TSLA.Close")
mean_tsla_close = mean(TSLA [, "TSLA.Close"])
var_tsla_close = variance(TSLA [, "TSLA.Close"])
median_deviation_tsla_close = median(TSLA [, "AAPL.Close"])
median_deviation_tsla_close = median(AAPL [, "AAPL.Close"])
model_lm_test = lm(TSLA.Close = AAPL.Close, data = compare_Stock)
install.packages("rmarkdown")
install.packages(tinytex)
install.packages("tinytex")
install.packages("readxl")
install.packages("moments")
#install packages
#load packages
#set WD (do it manually for now via R-Studio)
#import data
message("Importing relevant data")
market_return_df <- read.csv("Return/Market return.csv")
setwd("~/Documents/GitHub/SoSe-Sus-Fin")
#install packages
#load packages
#set WD (do it manually for now via R-Studio)
#import data
message("Importing relevant data")
market_return_df <- read.csv("Return/Market return.csv")
stock_return_df <- read.csv("Return/Stock return.csv")
rfr_df <- read.csv("Factors_Beta_RFR/Risk-free rate.csv")
ESG_Rating_A_df <- read.csv("ESG_Rating/ESG rating agency A.csv")
ESG_Rating_B_df <- read.csv("ESG_Rating/ESG rating agency B.csv")
ESG_Rating_C_df <- read.csv("ESG_Rating/ESG rating agency C.csv")
ESG_Rating_D_df <- read.csv("ESG_Rating/ESG rating agency D.csv")
ESG_Rating_E_df <- read.csv("ESG_Rating/ESG rating agency E.csv")
ESG_Rating_F_df <- read.csv("ESG_Rating/ESG rating agency F.csv")
stock_beta_df <- read.csv("Factors_Beta_RFR/Stock beta.csv")
cov_factors_df <- read.csv("Factors_Beta_RFR/Covariance_Factors.csv")
temp_market_excess_prep <- merge(market_return_df, rfr_df, by="Year")
temp_market_excess_prep$ex_ret <- temp_market_excess_prep$Market - temp_market_excess_prep$Risk.free.rate
#message("calculation done; doing data clean up")
keep_market_exret_columns <- c("Year", "ex_ret")
market_excess_return_df <- temp_market_excess_prep[, keep_market_exret_columns]
temp_stock_excess_prep <- merge(stock_return_df, rfr_df, by="Year")
View(temp_stock_excess_prep)
stock_excess_return_df <- temp_stock_excess_prep[, 2:501] - temp_stock_excess_prep$Risk.free.rate
# ==============================================================================
# 02_EXCESS_RETURNS.R
# ==============================================================================
# prepare market excess return
#message("calculating the market excess return")
## market excess return is the return of one stock - risk free rate
## used merge to do vector calculation
temp_market_excess_prep <- merge(market_return_df, rfr_df, by="Year")
temp_market_excess_prep$ex_ret <- temp_market_excess_prep$Market - temp_market_excess_prep$Risk.free.rate
#message("calculation done; doing data clean up")
keep_market_exret_columns <- c("Year", "ex_ret")
market_excess_return_df <- temp_market_excess_prep[, keep_market_exret_columns]
# prepare stock excess return
#message("calculating each individual stock excess return")
temp_stock_excess_prep <- merge(stock_return_df, rfr_df, by="Year")
# Vectorized calculation: subtracting the RFR vector from the returns matrix
## started from column two because the first column has the years
## not required in market excess returrn since after merge i just subtract one column
## with the other
stock_excess_return_df <- temp_stock_excess_prep[, 2:501] - temp_stock_excess_prep$Risk.free.rate
#message("Calculation done; doing data clean up")
View(stock_return_df)
View(stock_beta_df)
View(stock_beta_df)
View(stock_excess_return_df)
colnames(stock_excess_return_df) <- paste0(colnames(temp_stock_excess_prep)[2:501], "_excess")
View(stock_beta_df)
View(stock_excess_return_df)
View(ESG_Rating_B_df)
# ==============================================================================
# 03_STANDARDIZE_Z.R
# ==============================================================================
# Note, the for loop is from Gemini
# the temp variables are used to temporarily safe
# variables. those get overwritten in every new run
# Define the agency suffixes for the loop
agencies <- c("A", "B", "C", "D", "E", "F")
for (k in agencies) {
# 1. Fetch the raw data for the current agency (e.g., ESG_Rating_A_df)
temp_raw_df <- get(paste0("ESG_Rating_", k, "_df"))
# 2. Staging area: Isolate the Year and the 500 company return columns
## we divide here each dataframe between years (metadata) and the actual
## data, which needs to be transformed to get the z-score
temp_years_vec <- temp_raw_df[, 1]
temp_comp_mat  <- as.matrix(temp_raw_df[, 2:501])
# 3. Apply the Z-score calculation row-wise (Year by Year)
# scale() executes the standardization formula in two steps:
#   - Centering: Subtracts the mean (avg) of the 500 companies so the new mean is 0.
#   - Scaling: Divides by the standard deviation so the new spread is 1 unit.
## equation 7 is executed in the scale formula
## it is built up as follows
### transpose() results in putting stocks as columns and years as rows
### which is required due to how apply() works
### apply() when it processes a row of 500 stocks the output is a column
### 1 is a number to tell r, it needs to work across years (row wise)
### scale is the r way of caluclation the z score listed in the equation 7
# This makes different agency scales (e.g., 0-100 vs 1-5) directly comparable.
temp_z_mat <- t(apply(temp_comp_mat, 1, scale))
# 4. Implement the Equation (6) exception for Agency A
# The leading minus sign ensures high Z-scores always signal good performance.
if (k == "A") {
temp_z_mat <- -1 * temp_z_mat
}
# 5. Reconstruct the output with 'Year' as the first column
temp_final_df <- as.data.frame(cbind(Year = temp_years_vec, temp_z_mat))
# 6. Re-apply the original company names to the new columns
colnames(temp_final_df)[2:501] <- colnames(temp_raw_df)[2:501]
# 7. Assign to a permanent variable name (Z_A, Z_B, etc.) and save to environment
assign(paste0("Z_", k), temp_final_df)
}
# Clean up temporary "workbench" variables to keep the environment tidy
rm(list = ls(pattern = "temp_"), agencies, k)
# ==============================================================================
# 03_STANDARDIZE_Z.R
# ==============================================================================
# Note, the for loop is from Gemini
# the temp variables are used to temporarily safe
# variables. those get overwritten in every new run
# Define the agency suffixes for the loop
agencies <- c("A", "B", "C", "D", "E", "F")
for (k in agencies) {
# 1. Fetch the raw data for the current agency (e.g., ESG_Rating_A_df)
temp_raw_df <- get(paste0("ESG_Rating_", k, "_df"))
# 2. Staging area: Isolate the Year and the 500 company return columns
## we divide here each dataframe between years (metadata) and the actual
## data, which needs to be transformed to get the z-score
temp_years_vec <- temp_raw_df[, 1]
temp_comp_mat  <- as.matrix(temp_raw_df[, 2:501])
# 3. Apply the Z-score calculation row-wise (Year by Year)
# scale() executes the standardization formula in two steps:
#   - Centering: Subtracts the mean (avg) of the 500 companies so the new mean is 0.
#   - Scaling: Divides by the standard deviation so the new spread is 1 unit.
## equation 7 is executed in the scale formula
## it is built up as follows
### transpose() results in putting stocks as columns and years as rows
### which is required due to how apply() works
### apply() when it processes a row of 500 stocks the output is a column
### 1 is a number to tell r, it needs to work across years (row wise)
### scale is the r way of caluclation the z score listed in the equation 7
# This makes different agency scales (e.g., 0-100 vs 1-5) directly comparable.
temp_z_mat <- t(apply(temp_comp_mat, 1, scale))
# 4. Implement the Equation (6) exception for Agency A
# The leading minus sign ensures high Z-scores always signal good performance.
if (k == "A") {
temp_z_mat <- -1 * temp_z_mat
}
# 5. Reconstruct the output with 'Year' as the first column
temp_final_df <- as.data.frame(cbind(Year = temp_years_vec, temp_z_mat))
# 6. Re-apply the original company names to the new columns
colnames(temp_final_df)[2:501] <- colnames(temp_raw_df)[2:501]
# 7. Assign to a permanent variable name (Z_A, Z_B, etc.) and save to environment
## basically each calculated ESG Score gets its own unique datframe with
## standardized z values
assign(paste0("Z_", k), temp_final_df)
}
# Clean up temporary "workbench" variables to keep the environment tidy
rm(list = ls(pattern = "temp_"), agencies, k)
#message("Task 3.2 done; removing unnessessary dataframes")
rm(ESG_Rating_A_df, ESG_Rating_B_df, ESG_Rating_C_df,
ESG_Rating_D_df, ESG_Rating_E_df, ESG_Rating_F_df)
## essentially you overlay all the 6 dataframes ontop of each other
## since their structure is essentially the same and sum up the ratings
## to one dataframe
## at the end you need to divide it by 6 to get the average
temp_s_matrix <- Reduce("+", temp_z_list) / 6
# ==============================================================================
# 04_COMBINE_ESG.R
# ==============================================================================
# Note, this codepart is also from Gemini
# 1. Create a list of just the numeric parts (excluding Year) of your 6 Z-matrices
# We use columns 2:501 for each dataframe created in the previous step
temp_z_list <- list(Z_A[, 2:501], Z_B[, 2:501], Z_C[, 2:501],
Z_D[, 2:501], Z_E[, 2:501], Z_F[, 2:501])
# 2. Calculate the average across agencies (Equation 8)
# Reduce(+) sums the dataframes element-wise; then we divide by 6
## essentially you overlay all the 6 dataframes ontop of each other
## since their structure is essentially the same and sum up the ratings
## to one dataframe
## at the end you need to divide it by 6 to get the average
temp_s_matrix <- Reduce("+", temp_z_list) / 6
# 3. Final Normalization (Equation 9)
# We apply scale() row-wise again to ensure the final aggregate is a Z-score
# This ensures mean = 0 and SD = 1 for the final combined rating
## for a detailed explaination as to what happened heare
## check 3_2
temp_combined_z <- t(apply(temp_s_matrix, 1, scale))
# 4. Construct the final dataframe with Year in the first column
ESG_combined_df <- as.data.frame(cbind(Year = Z_A$Year, temp_combined_z))
# 5. Restore company names
colnames(ESG_combined_df)[2:501] <- colnames(Z_A)[2:501]
# Final Cleanup - removing the individual agency Z-scores to free up RAM
# message("Task 3.3 done")
rm(list = ls(pattern = "temp_"))
# Task states having one matrix of 25x500.
# for explaination as to why i have 26x501 check notes in 3.2
View(ESG_combined_df)
# ==============================================================================
# 04_FINAL_ESG_METRICS.R
# ==============================================================================
# Note, this code snippet comes from Gemini
# if some coments look confusing, the reason for that
# is that gemini did redundant work in script 3.3 and here
# therefore i just deleted the redundant part
#ESG Uncertainty (Equation 10) ---
# We need the standard deviation across the 6 agencies for each company-year cell.
# We stack the matrices into a 3D array to calculate SD across the 'agency' dimension
## seems like instead of using reduce() as in equation 8 hiere building a cube
## is better.
## reduce() is for sequentilal math.
temp_array <- simplify2array(list(as.matrix(Z_A[,-1]), as.matrix(Z_B[,-1]),
as.matrix(Z_C[,-1]), as.matrix(Z_D[,-1]),
as.matrix(Z_E[,-1]), as.matrix(Z_F[,-1])))
## what essentially happens here is now we dont need an average as stated
## and just the standard deviation across the rating agencies
## gemini therefore deducted by using a cube and overlaying all z matrixes
## and using the apply, it takes a lot of work off our shoulders
### here we are looking for the spread of the normal distribution
### equation 8 looked for the middle of the stack (normalverteilung)
# apply(..., c(1,2), sd) calculates SD across the 6 agencies for every cell
## standard deviation of z-scores across rating agencies
temp_u_mat <- apply(temp_array, c(1, 2), sd)
# Create final uncertainty dataframe
ESGU_combined_df <- as.data.frame(cbind(Year = Z_A$Year, temp_u_mat))
colnames(ESGU_combined_df)[2:501] <- colnames(Z_A)[2:501]
# Final Cleanup
rm(list = ls(pattern = "temp_"))
message("Task 3.4 done; removing unnecessary dataframes")
# Note: Per your code, this removes the rating dataframes
rm(Z_A, Z_B, Z_C, Z_D, Z_E, Z_F)
# also gonna clean the current values in cache after finishing a whole task
rm(all_ratings_vector, current_year, i, keep_market_exret_columns, unique_years)
# Task states having a 25x500 matrix
# as to why i have a 26x501 see explaination in 3.2
#install packages
#load packages
#set WD (do it manually for now via R-Studio)
#import data
message("Importing relevant data")
market_return_df <- read.csv("Return/Market return.csv")
stock_return_df <- read.csv("Return/Stock return.csv")
rfr_df <- read.csv("Factors_Beta_RFR/Risk-free rate.csv")
ESG_Rating_A_df <- read.csv("ESG_Rating/ESG rating agency A.csv")
ESG_Rating_B_df <- read.csv("ESG_Rating/ESG rating agency B.csv")
ESG_Rating_C_df <- read.csv("ESG_Rating/ESG rating agency C.csv")
ESG_Rating_D_df <- read.csv("ESG_Rating/ESG rating agency D.csv")
ESG_Rating_E_df <- read.csv("ESG_Rating/ESG rating agency E.csv")
ESG_Rating_F_df <- read.csv("ESG_Rating/ESG rating agency F.csv")
stock_beta_df <- read.csv("Factors_Beta_RFR/Stock beta.csv")
cov_factors_df <- read.csv("Factors_Beta_RFR/Covariance_Factors.csv")
message("Start Task 3.1")
source("Task3_Code_Snippets/Task3_1_excess_ret.R")
message("Start Task 3.2")
source("Task3_Code_Snippets/Task3_2_z_score.R")
message("Start Task 3.3")
source("Task3_Code_Snippets/Task3_3_esg_combination.R")
source("Summary_Code_Snippets/Task3_3_std_dev_mean.R")
message("Task 3.3 done; Starting Task 3.4")
source("Task3_Code_Snippets/Task3_4_esg_combined_uncertainty.R")
# before going into task 4 and 5 it ssems like the data on market excess
# as well as stock excess was still from 1927. so data prep is needed
# stock excess return is needed as a 2000 to 2025 since Task 4 deals with
# a datalag to get results between 2001 and 2025
# market excess return is shortened to 2001 since in task 5 we probably
# need to do a matrix multiplication so it would be wise to get the data
# cut off and set between 2001 and 2025
# cut off and set between 2001 and 2025
# ---- Add in ------
# due to task 8 neeeding data from 1927 to 2000 i need to create a new dataframe before shorting the old one
stock_excess_return_pre_2000_df <- stock_excess_return_df[stock_excess_return_df$Year >= 1927 &
stock_excess_return_df$Year <= 2000, ]
# --------------------
stock_excess_return_df <- stock_excess_return_df[stock_excess_return_df$Year >= 2000 &
stock_excess_return_df$Year <= 2025, ]
market_excess_return_df <- market_excess_return_df[market_excess_return_df$Year >= 2001 &
market_excess_return_df$Year <= 2025, ]
message("Start Beta Sorting")
# --- 1. DATA ALIGNMENT (The "Muppet" Check) ---
# We have data from 2000 to 2025 (26 rows).
# To follow the 'previous year' rule:
#   - We need signals from 2000 to 2024 (25 years)
#   - We need returns from 2001 to 2025 (25 years)
## to be honest, even when going through it again it is
## still not 100% clear how the dataframe should have
## been constructed
## as stated the base is the prevous years beta which are sorted
## the weighting comes from the returns on that year
# Extracting the 25 years of signals (t-1)
# We take rows 1 to 25 of the Beta data (Year 2000 to 2024)
signal_betas_df <- stock_beta_df[stock_beta_df$Year >= 2000 & stock_beta_df$Year <= 2024, ]
# Extracting the 25 years of performance (t)
# We take rows that match 2001 to 2025
performance_returns_df <- stock_excess_return_df[stock_excess_return_df$Year >= 2001 & stock_excess_return_df$Year <= 2025, ]
# --- 2. INITIALIZATION ---
# Create a matrix to hold returns for 5 portfolios across 25 years
beta_port_matrix <- matrix(NA, nrow = 25, ncol = 5)
colnames(beta_port_matrix) <- c("P1_Low_Beta", "P2", "P3", "P4", "P5_High_Beta")
View(signal_betas_df)
#install packages
#load packages
#set WD (do it manually for now via R-Studio)
#import data
message("Importing relevant data")
market_return_df <- read.csv("Return/Market return.csv")
stock_return_df <- read.csv("Return/Stock return.csv")
rfr_df <- read.csv("Factors_Beta_RFR/Risk-free rate.csv")
ESG_Rating_A_df <- read.csv("ESG_Rating/ESG rating agency A.csv")
ESG_Rating_B_df <- read.csv("ESG_Rating/ESG rating agency B.csv")
ESG_Rating_C_df <- read.csv("ESG_Rating/ESG rating agency C.csv")
ESG_Rating_D_df <- read.csv("ESG_Rating/ESG rating agency D.csv")
ESG_Rating_E_df <- read.csv("ESG_Rating/ESG rating agency E.csv")
ESG_Rating_F_df <- read.csv("ESG_Rating/ESG rating agency F.csv")
stock_beta_df <- read.csv("Factors_Beta_RFR/Stock beta.csv")
cov_factors_df <- read.csv("Factors_Beta_RFR/Covariance_Factors.csv")
message("Start Task 3.1")
source("Task3_Code_Snippets/Task3_1_excess_ret.R")
message("Start Task 3.2")
source("Task3_Code_Snippets/Task3_2_z_score.R")
message("Start Task 3.3")
source("Task3_Code_Snippets/Task3_3_esg_combination.R")
source("Summary_Code_Snippets/Task3_3_std_dev_mean.R")
message("Task 3.3 done; Starting Task 3.4")
source("Task3_Code_Snippets/Task3_4_esg_combined_uncertainty.R")
# before going into task 4 and 5 it ssems like the data on market excess
# as well as stock excess was still from 1927. so data prep is needed
# stock excess return is needed as a 2000 to 2025 since Task 4 deals with
# a datalag to get results between 2001 and 2025
# market excess return is shortened to 2001 since in task 5 we probably
# need to do a matrix multiplication so it would be wise to get the data
# cut off and set between 2001 and 2025
# ---- Add in ------
# due to task 8 neeeding data from 1927 to 2000 i need to create a new dataframe before shorting the old one
stock_excess_return_pre_2000_df <- stock_excess_return_df[stock_excess_return_df$Year >= 1927 &
stock_excess_return_df$Year <= 2000, ]
# --------------------
stock_excess_return_df <- stock_excess_return_df[stock_excess_return_df$Year >= 2000 &
stock_excess_return_df$Year <= 2025, ]
market_excess_return_df <- market_excess_return_df[market_excess_return_df$Year >= 2001 &
market_excess_return_df$Year <= 2025, ]
message("Start Task 4 Portfolio Building")
source("Task4_Sorting_Code_Snippets/Task4_1_beta_sorting.R")
source("Summary_Code_Snippets/Task4_1_beta_summary.R")
View(Sorted_Beta_Results_Summary_df)
View(Sorted_Beta_Portfolio_Results_df)
View(Sorted_Beta_Portfolio_Results_df)
